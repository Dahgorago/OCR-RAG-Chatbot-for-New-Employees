services:
  ocr_api:
    build:
      context: ./OCR
      dockerfile: Dockerfile
    ports:
      - 8001:8001
    volumes:
      - ./app:/Dockerize/OCR/app/
      - ocr_output_data:/OCR/
    networks:
      - intern
    container_name: ocr_api
    command: uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
  qdrant:
    image: qdrant/qdrant
    ports:
      - 6333:6333
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - intern
    container_name: qdrant_db

  qdrant_api:
    build:
      context: ./Qdrant
      dockerfile: Dockerfile
    ports:
      - 8002:8002
    depends_on:
      - qdrant
    volumes:
      - ./app:/Dockerize/Qdrant/app
      - ocr_output_data:/OCR/
    networks:
      - intern
    container_name: qdrant_api
    command: uvicorn app.main:app --host 0.0.0.0 --port 8002 --reload
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  llm_api:
    build:
      context: ./LLM
      dockerfile: Dockerfile
    ports:
      - 8003:8003
    depends_on:
      - ollama
      - qdrant
    volumes:
      - ./app:/Dockerize/LLM/app
    networks:
      - intern
    container_name: llm_api
    command: uvicorn app.main:app --host 0.0.0.0 --port 8003 --reload
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  ollama:
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    volumes:
      - .:/code
      - ./ollama/ollama:/root/.ollama
    container_name: ollama_api
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=-1h
      - OLLAMA_HOST=0.0.0.0
      - CUDA_VISIBLE_DEVICES=0,1,2
      - OLLAMA_SCHED_SPREAD=true
    networks:
      - intern
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
      
  chainlit_api:
    build:
      context: ./Chainlit
      dockerfile: Dockerfile
    ports:
      - 8004:8004
    volumes:
      - ./app:/Dockerize/Chainlit/app/
    networks:
      - intern
    container_name: chainlit_api
    command: chainlit run app/app.py -w --port 8004
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  intern:
    driver: bridge

volumes:
  qdrant_data:
  ocr_output_data: